{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3236396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Selenium WebDriver\n",
      "WebDriver: The core component of Selenium, it provides a programming interface for driving the browser to interact with web elements.\n",
      "2. Selenium IDE\n",
      "IDE (Integrated Development Environment): A Firefox and Chrome browser extension used for rapid prototyping of test cases.\n",
      "3. Selenium Grid\n",
      "Grid: A server that allows tests to run on different machines against different browsers simultaneously, enabling parallel testing.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.Study of Python and basic commands to access text data. (from notepad, pdf, word documents,online)\n",
    "\n",
    "# Open a text file and read its contents\n",
    "with open('/home/harshitha/Downloads/nlp lab/harshinlaplab', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66516d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "\n",
    "# Simple example to fetch a webpage\n",
    "conn = http.client.HTTPConnection('example.com')\n",
    "conn.request('GET', '/')\n",
    "\n",
    "response = conn.getresponse()\n",
    "print(response.status, response.reason)\n",
    "\n",
    "data = response.read()\n",
    "print(data.decode())\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af96f6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  The weather was beautiful. I went for a walk i...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  weather beautiful went walk park sunny day bir...  \n"
     ]
    }
   ],
   "source": [
    "#2.Perform text pre - processing on a given corpus without using any pre -defined NLP packages.\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample data creation (replace this with your actual data loading logic)\n",
    "data = {'content': [\"The weather was beautiful. I went for a walk in the park. It was a sunny day, and the birds were chirping happily. Suddenly, a black cat crossed my path. I stopped and watched it disappear into the bushes. After that, I continued my stroll, enjoying the tranquility of nature.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set([\"is\", \"an\", \"the\", \"this\", \"another\", \"i\", \"it\", \"was\", \"in\", \"for\", \"a\", \"and\", \"of\", \"my\"]) # Add more as needed\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming (using a simple example)\n",
    "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
    "    # Lemmatization (using a simple example)\n",
    "    tokens = [word[:-1] if word.endswith('s') else word for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the cleaning function to the DataFrame\n",
    "df['cleaned_content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "669a30e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               cleaned_content                                         bigrams\n",
      "0  this is an example sentence  [this is, is an, an example, example sentence]\n",
      "1     another example sentence             [another example, example sentence]\n"
     ]
    }
   ],
   "source": [
    "#3.)Implement N -Gram model in python without using any predefined NLPpackages. Note: use corpus of your own choice\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {'cleaned_content': [\"this is an example sentence\", \"another example sentence\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = text.split()\n",
    "    ngrams_list = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    return ngrams_list\n",
    "\n",
    "# Apply the function to generate bigrams\n",
    "df['bigrams'] = df['cleaned_content'].apply(generate_ngrams, n=2)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dbb4b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                            cleaned_content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         pos_tags\n",
      "0  The cat chased the mouse around the house. Birds sang in the trees while the sun shone brightly in the sky. A group of children played happily in the park, laughing and running around.  [(The, Noun), (cat, Noun), (chased, Noun), (the, Noun), (mouse, Noun), (around, Noun), (the, Noun), (house., Noun), (Birds, Noun), (sang, Noun), (in, Noun), (the, Noun), (trees, Noun), (while, Noun), (the, Noun), (sun, Noun), (shone, Noun), (brightly, Adverb), (in, Noun), (the, Noun), (sky., Noun), (A, Noun), (group, Noun), (of, Noun), (children, Noun), (played, Noun), (happily, Adverb), (in, Noun), (the, Noun), (park,, Noun), (laughing, Verb), (and, Noun), (running, Verb), (around., Noun)]\n"
     ]
    }
   ],
   "source": [
    "#4. # Implement Part-of-Speech (POS) Tagging.\n",
    "# \"The cat chased the mouse around the house. Birds sang in the trees while the\n",
    "# sun shone brightly in the sky. A group of children played happily in the park,\n",
    "# laughing and running around.â€\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data creation with provided text\n",
    "data = {'cleaned_content': [\n",
    "    \"The cat chased the mouse around the house. Birds sang in the trees while the sun shone brightly in the sky. A group of children played happily in the park, laughing and running around.\"\n",
    "]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to generate simple POS tags (Noun, Verb, Adverb)\n",
    "def simple_pos_tagging(text):\n",
    "    tokens = text.split()\n",
    "    pos_tags = []\n",
    "    for token in tokens:\n",
    "        if token.endswith('ing'):\n",
    "            pos_tags.append((token, 'Verb'))\n",
    "        elif token.endswith('ly'):\n",
    "            pos_tags.append((token, 'Adverb'))\n",
    "        else:\n",
    "            pos_tags.append((token, 'Noun'))\n",
    "    return pos_tags\n",
    "\n",
    "# Apply the function to generate POS tags\n",
    "df['pos_tags'] = df['cleaned_content'].apply(simple_pos_tagging)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df.to_string(max_colwidth=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d37695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            POS_tags  \\\n",
      "0  [(this, Noun), (is, Noun), (an, Noun), (exampl...   \n",
      "1  [(another, Noun), (example, Noun), (sentence, ...   \n",
      "\n",
      "                    noun_phrases  \n",
      "0  [this is an example sentence]  \n",
      "1     [another example sentence]  \n"
     ]
    }
   ],
   "source": [
    "#5.Implement chunking to extract Noun Phrases.\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data creation\n",
    "data = {\n",
    "    'POS_tags': [\n",
    "        [('this', 'Noun'), ('is', 'Noun'), ('an', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')],\n",
    "        [('another', 'Noun'), ('example', 'Noun'), ('sentence', 'Noun')]\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to perform simple noun phrase chunking\n",
    "def simple_noun_phrase_chunking(pos_tags):\n",
    "    noun_phrases = []\n",
    "    current_phrase = []\n",
    "    for token, tag in pos_tags:\n",
    "        if tag in ['Noun', 'Adjective']:\n",
    "            current_phrase.append(token)\n",
    "        elif current_phrase:\n",
    "            noun_phrases.append(' '.join(current_phrase))\n",
    "            current_phrase = []\n",
    "    if current_phrase:\n",
    "        noun_phrases.append(' '.join(current_phrase))\n",
    "    return noun_phrases\n",
    "\n",
    "# Apply the function to generate noun phrases\n",
    "df['noun_phrases'] = df['POS_tags'].apply(simple_noun_phrase_chunking)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b8bc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Completions:\n",
      "- After a long day at work, I like to relax by watching my favorite TV show\n",
      "- After a long day at work, I like to relax by going for a walk\n",
      "- After a long day at work, I like to relax by reading a book\n",
      "- After a long day at work, I like to relax by enjoying a cup of tea\n"
     ]
    }
   ],
   "source": [
    "#6.Sentence completion with words or phrases using random prompts.\n",
    "import random\n",
    "\n",
    "# Dictionary with sentence prompts and possible completions\n",
    "sentence_prompts = {\n",
    "    \"She opened the door and saw a\": [\"beautiful garden\", \"mysterious figure\", \"bright light\"],\n",
    "    \"After a long day at work, I like to relax by\": [\"watching my favorite TV show\", \"going for a walk\", \"reading a book\"]\n",
    "}\n",
    "\n",
    "input_prompt = \"After a long day at work, I like to relax by\"\n",
    "\n",
    "if input_prompt in sentence_prompts:\n",
    "    possible_completions = sentence_prompts[input_prompt]\n",
    "    print(\"Possible Completions:\")\n",
    "    for completion in possible_completions:\n",
    "        print(f\"- {input_prompt} {completion}\")\n",
    "else:\n",
    "    print(\"Prompt not found in the dictionary.\")\n",
    "\n",
    "# Use random to create a random sentence completion\n",
    "random_completion = random.choice([\"enjoying a cup of tea\", \"listening to music\", \"playing video games\"])\n",
    "print(f\"- {input_prompt} {random_completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e8229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product!\n",
      "Label: positive\n",
      "\n",
      "Text: It's terrible.\n",
      "Label: negative\n",
      "\n",
      "Text: Neutral statement.\n",
      "Label: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7.Implement machine learning sentiment classification without using any pre defined NLP packages:\n",
    "'''Training Corpus:\n",
    "(\"I love this product\", \"positive\"),\n",
    "(\"This is excellent\", \"positive\"),\n",
    "(\"Terrible service\", \"negative\"),\n",
    "(\"It's okay, not great\", \"neutral\"),\n",
    "(\"Amazing experience\", \"positive\"),\n",
    "(\"Disappointing outcome\", \"negative\"),\n",
    "(\"Neutral feelings\", \"neutral\"),\n",
    "(\"I dislike it\", \"negative\")'''\n",
    "\n",
    "# Sample data\n",
    "data = [\"I love this product!\", \"It's terrible.\", \"Neutral statement.\"]\n",
    "\n",
    "# Training corpus\n",
    "training_corpus = [\n",
    "    (\"I love this product\", \"positive\"),\n",
    "    (\"This is excellent\", \"positive\"),\n",
    "    (\"Terrible service\", \"negative\"),\n",
    "    (\"It's okay, not great\", \"neutral\"),\n",
    "    (\"Amazing experience\", \"positive\"),\n",
    "    (\"Disappointing outcome\", \"negative\"),\n",
    "    (\"Neutral feelings\", \"neutral\"),\n",
    "    (\"I dislike it\", \"negative\")\n",
    "]\n",
    "\n",
    "# Function to determine sentiment label\n",
    "def determine_sentiment_label(text):\n",
    "    # Check using predefined keywords\n",
    "    if \"love\" in text.lower():\n",
    "        return 'positive'\n",
    "    elif \"terrible\" in text.lower():\n",
    "        return 'negative'\n",
    "    else:\n",
    "        # Check using the training corpus\n",
    "        for corpus_text, label in training_corpus:\n",
    "            if corpus_text.lower() in text.lower():\n",
    "                return label\n",
    "        # Default to neutral if no match found\n",
    "        return 'neutral'\n",
    "\n",
    "# Create result dictionary\n",
    "result_dict = {'text': data, 'label': [determine_sentiment_label(text) for text in data]}\n",
    "\n",
    "# Print results\n",
    "for text, label in zip(result_dict['text'], result_dict['label']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b964d434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product!\n",
      "Label: positive\n",
      "\n",
      "Text: It's terrible.\n",
      "Label: negative\n",
      "\n",
      "Text: Neutral statement.\n",
      "Label: neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#or 7b.\n",
    "data = [\"I love this product!\", \"It's terrible.\", \"Neutral statement.\"]\n",
    "\n",
    "def determine_sentiment_label(text):\n",
    "    if \"love\" in text.lower():\n",
    "        return 'positive'\n",
    "    elif \"terrible\" in text.lower():\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "result_dict = {'text': data, 'label': [determine_sentiment_label(text) for text in data]}\n",
    "\n",
    "for text, label in zip(result_dict['text'], result_dict['label']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Label: {label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b5dbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Summary:\n",
      "By integrating advanced\n",
      " natural language processing (NLP) techniques and machine learning algorithms, the project aims to develop a robust sentiment analysis\n",
      "  framework capable of delivering accurate insights across diverse platforms. Ultimately,\n",
      "    these advancements are expected to empower businesses, marketers, and researchers with more reliable tools for making informed\n",
      "     decisions based on a deeper understanding of public sentiment and emotional trends. This project aims to enhance sentiment analysis by addressing critical challenges such as interpreting\n",
      " figurative language, including sarcasm and irony, which often confound existing models\n"
     ]
    }
   ],
   "source": [
    "#8.Text Summarization (Extractive and Abstractive)\n",
    "def simple_summarization(article, num_sentences=3):\n",
    "    sentences = article.split(\".\")\n",
    "    # Remove empty strings and strip leading/trailing whitespace\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "    # Calculate the importance score for each sentence (based on sentence length)\n",
    "    scores = [len(sentence) for sentence in sentences]\n",
    "\n",
    "    # Select the top N sentences with the highest importance scores\n",
    "    selected_sentences = sorted(zip(sentences, scores), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "\n",
    "    # Extract the selected sentences\n",
    "    summary = [sentence for sentence, _ in selected_sentences]\n",
    "\n",
    "    # Join the selected sentences into a summary paragraph\n",
    "    return '. '.join(summary)\n",
    "\n",
    "# Example usage\n",
    "article_text = \"\"\"\n",
    "Sentiment analysis is increasingly vital in modern data-driven contexts, providing crucial insights into public opinion and emotional\n",
    " trends from vast textual datasets. This project aims to enhance sentiment analysis by addressing critical challenges such as interpreting\n",
    " figurative language, including sarcasm and irony, which often confound existing models. Furthermore, the project seeks to expand the scope\n",
    " of emotional analysis beyond traditional categories, aiming for a more nuanced understanding of human emotions. By integrating advanced\n",
    " natural language processing (NLP) techniques and machine learning algorithms, the project aims to develop a robust sentiment analysis\n",
    "  framework capable of delivering accurate insights across diverse platforms. Emphasis is placed on understanding platform-specific\n",
    "   sentiment dynamics to ensure consistent and relevant analyses across different social media and digital platforms. Ultimately,\n",
    "    these advancements are expected to empower businesses, marketers, and researchers with more reliable tools for making informed\n",
    "     decisions based on a deeper understanding of public sentiment and emotional trends.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a summary of the article\n",
    "summary = simple_summarization(article_text)\n",
    "print(\"Article Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21576e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person List: ['John Smith', 'Elon Musk']\n",
      "Place List: ['France', 'Paris', 'Tokyo', 'Shibuya Crossing']\n"
     ]
    }
   ],
   "source": [
    "#9.Perform Name Entity Recognition (NER) on given corpus text.\n",
    "'''The capital of [France] is [Paris], a city known for its iconic [Eiffel Tower].\n",
    "[John Smith] visited [Tokyo] last summer, exploring the bustling streets of\n",
    "[Shibuya Crossing]. [May 5th, 2023] marks the anniversary of a significant\n",
    "event in [history]. [Elon Musk] is the CEO of [SpaceX] and [Tesla].'''\n",
    "import re\n",
    "\n",
    "# Corpus text\n",
    "corpus_text = \"The capital of [France] is [Paris], a city known for its iconic [Eiffel Tower]. [John Smith] visited [Tokyo] last summer, exploring the bustling streets of [Shibuya Crossing]. [May 5th, 2023] marks the anniversary of a significant event in [history]. [Elon Musk] is the CEO of [SpaceX] and [Tesla].\"\n",
    "\n",
    "# Initialize lists\n",
    "person_list = []\n",
    "place_list = []\n",
    "\n",
    "# Define regex pattern to find entities in brackets\n",
    "pattern = r'\\[(.*?)\\]'\n",
    "\n",
    "# Find all matches in the corpus text\n",
    "matches = re.findall(pattern, corpus_text)\n",
    "\n",
    "# Extract entities and populate lists\n",
    "for entity in matches:\n",
    "    if any(name in entity for name in [\"Barack Obama\", \"John Smith\", \"Elon Musk\"]):\n",
    "        person_list.append(entity)\n",
    "    elif any(place in entity for place in [\"Hawaii\", \"United States\", \"France\", \"Paris\", \"Tokyo\", \"Shibuya Crossing\"]):\n",
    "        place_list.append(entity)\n",
    "\n",
    "# Print the lists\n",
    "print(\"Person List:\", person_list)\n",
    "print(\"Place List:\", place_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0389b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['The', 'quick', 'brown', 'foxes', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary's\", 'cat', 'is', 'playing', 'with', 'a', 'ball.', 'Running', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'houses', 'lined', 'the', 'street,', 'attracting', 'curious', 'onlookers.']\n",
      "Stemmed words: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary'\", 'cat', 'i', 'play', 'with', 'a', 'ball.', 'Runn', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'hous', 'lined', 'the', 'street,', 'attract', 'curiou', 'onlookers.']\n",
      "Lemmatized words: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dogs.', \"Mary'\", 'cat', 'i', 'play', 'with', 'a', 'ball.', 'Runn', 'swiftly,', 'the', 'athlete', 'won', 'the', 'race.', 'The', 'painted', 'hous', 'lined', 'the', 'street,', 'attract', 'curiou', 'onlookers.']\n",
      "\n",
      "Word: misunderstanding\n",
      "Morphemes: ['mis', 'understand', 'ing']\n"
     ]
    }
   ],
   "source": [
    "#10.Perform Morphological analysis without using any pre defined NLP packages\n",
    "'''The text corpus given below :\n",
    "The quick brown foxes jumped over the lazy dogs. Mary's cat is playing with\n",
    "a ball. Running swiftly, the athlete won the race. The painted houses lined the\n",
    "street, attracting curious onlookers.'''\n",
    "\n",
    "# Tokenizer function\n",
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Stemming function\n",
    "def simple_porter_stemmer(word):\n",
    "    # A simple stemming function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "# Lemmatization function\n",
    "def simple_wordnet_lemmatizer(word):\n",
    "    # A simple lemmatization function (for illustration purposes)\n",
    "    if word.endswith(\"es\"):\n",
    "        return word[:-2]\n",
    "    elif word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    return word\n",
    "\n",
    "# Function to analyze morphemes\n",
    "def analyze_morphemes(word, prefixes, root, suffixes):\n",
    "    morphemes = []\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            morphemes.append(prefix)\n",
    "            word = word[len(prefix):]\n",
    "    morphemes.append(root)\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            morphemes.append(suffix)\n",
    "            word = word[:-len(suffix)]\n",
    "    return morphemes\n",
    "\n",
    "# Text corpus\n",
    "text = \"\"\"The quick brown foxes jumped over the lazy dogs. Mary's cat is playing with\n",
    "a ball. Running swiftly, the athlete won the race. The painted houses lined the\n",
    "street, attracting curious onlookers.\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = simple_tokenizer(text)\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "stemmed_words = [simple_porter_stemmer(word) for word in words]\n",
    "lemmatized_words = [simple_wordnet_lemmatizer(word) for word in words]\n",
    "\n",
    "# Print original, stemmed, and lemmatized words\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "print(\"Lemmatized words:\", lemmatized_words)\n",
    "print()\n",
    "\n",
    "# Morpheme analysis example\n",
    "word = \"misunderstanding\"\n",
    "prefixes = [\"mis\"]\n",
    "root = \"understand\"\n",
    "suffixes = [\"ing\"]\n",
    "morphemes = analyze_morphemes(word, prefixes, root, suffixes)\n",
    "\n",
    "# Print morphemes\n",
    "print(\"Word:\", word)\n",
    "print(\"Morphemes:\", morphemes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc8c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/harshitha/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: transformers in /home/harshitha/.local/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: joblib in /home/harshitha/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/harshitha/.local/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/harshitha/.local/lib/python3.10/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: click in /home/harshitha/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/harshitha/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/harshitha/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/harshitha/.local/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: filelock in /home/harshitha/.local/lib/python3.10/site-packages (from transformers) (3.15.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/harshitha/.local/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: requests in /home/harshitha/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/harshitha/.local/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/harshitha/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/harshitha/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/harshitha/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7abc5004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is AI?\n",
      "Answer: \n",
      "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines\n",
      "\n",
      "Question: What are the core problems of artificial intelligence?\n",
      "Answer: The core problems of artificial intelligence include programming computers \n",
      "for certain traits such as knowledge, reasoning, problem solving, perception, learning, planning, \n",
      "and the ability to manipulate and move objects.\n",
      "\n",
      "\n",
      "Question: What is AI a branch of?\n",
      "Answer: \n",
      "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to find answers in a paragraph\n",
    "def find_answers(paragraph, questions):\n",
    "    answers = []\n",
    "\n",
    "    # Split the paragraph into sentences\n",
    "    sentences = paragraph.split('. ')\n",
    "    \n",
    "    # Iterate over the questions\n",
    "    for question in questions:\n",
    "        # Find a sentence that contains the most keywords from the question\n",
    "        best_sentence = ''\n",
    "        max_keyword_matches = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Count the number of keywords from the question in the sentence\n",
    "            keyword_matches = sum(1 for word in question.split() if word.lower() in sentence.lower())\n",
    "\n",
    "            if keyword_matches > max_keyword_matches:\n",
    "                max_keyword_matches = keyword_matches\n",
    "                best_sentence = sentence\n",
    "\n",
    "        answers.append(best_sentence)\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# Sample paragraph (corpus)\n",
    "paragraph = \"\"\"\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines. \n",
    "It has become an essential part of the technology industry. Research associated with artificial intelligence \n",
    "is highly technical and specialized. The core problems of artificial intelligence include programming computers \n",
    "for certain traits such as knowledge, reasoning, problem solving, perception, learning, planning, \n",
    "and the ability to manipulate and move objects.\n",
    "\"\"\"\n",
    "\n",
    "# Sample questions\n",
    "questions = [\n",
    "    \"What is AI?\",\n",
    "    \"What are the core problems of artificial intelligence?\",\n",
    "    \"What is AI a branch of?\",\n",
    "]\n",
    "\n",
    "# Get answers to the questions\n",
    "answers = find_answers(paragraph, questions)\n",
    "\n",
    "# Print the questions and answers\n",
    "for question, answer in zip(questions, answers):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f5ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial: 3\n",
      "intelligence: 3\n",
      "ai: 1\n",
      "is: 2\n",
      "a: 1\n",
      "branch: 1\n",
      "of: 3\n",
      "computer: 1\n",
      "science: 1\n",
      "that: 1\n",
      "aims: 1\n",
      "to: 2\n",
      "create: 1\n",
      "intelligent: 1\n",
      "machines: 1\n",
      "it: 1\n",
      "has: 1\n",
      "become: 1\n",
      "an: 1\n",
      "essential: 1\n",
      "part: 1\n",
      "the: 3\n",
      "technology: 1\n",
      "industry: 1\n",
      "research: 1\n",
      "associated: 1\n",
      "with: 1\n",
      "highly: 1\n",
      "technical: 1\n",
      "and: 3\n",
      "specialized: 1\n",
      "core: 1\n",
      "problems: 1\n",
      "include: 1\n",
      "programming: 1\n",
      "computers: 1\n",
      "for: 1\n",
      "certain: 1\n",
      "traits: 1\n",
      "such: 1\n",
      "as: 1\n",
      "knowledge: 1\n",
      "reasoning: 1\n",
      "problem: 1\n",
      "solving: 1\n",
      "perception: 1\n",
      "learning: 1\n",
      "planning: 1\n",
      "ability: 1\n",
      "manipulate: 1\n",
      "move: 1\n",
      "objects: 1\n"
     ]
    }
   ],
   "source": [
    "#count freq of each word in corpus\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "\n",
    "# Function to count word frequencies in a paragraph\n",
    "def count_word_frequencies(paragraph):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    paragraph = re.sub(r'[^\\w\\s]', '', paragraph).lower()\n",
    "    \n",
    "    # Split paragraph into words\n",
    "    words = paragraph.split()\n",
    "    \n",
    "    # Count the frequency of each word\n",
    "    word_frequencies = Counter(words)\n",
    "    \n",
    "    return word_frequencies\n",
    "\n",
    "# Sample paragraph (corpus)\n",
    "paragraph = \"\"\"\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines. \n",
    "It has become an essential part of the technology industry. Research associated with artificial intelligence \n",
    "is highly technical and specialized. The core problems of artificial intelligence include programming computers \n",
    "for certain traits such as knowledge, reasoning, problem solving, perception, learning, planning, \n",
    "and the ability to manipulate and move objects.\n",
    "\"\"\"\n",
    "\n",
    "# Get word frequencies\n",
    "word_frequencies = count_word_frequencies(paragraph)\n",
    "\n",
    "# Print the word frequencies\n",
    "for word, frequency in word_frequencies.items():\n",
    "    print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe73e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines.\n",
      "Subject: (AI), Verb: is, Object: a\n",
      "Adjective: intelligent, Noun: machines.\n",
      "Sentence: \n",
      "It has become an essential part of the technology industry.\n",
      "Adjective: essential, Noun: part\n",
      "Sentence: Research associated with artificial intelligence \n",
      "is highly technical and specialized.\n",
      "Subject: intelligence, Verb: is, Object: highly\n",
      "Adjective: technical, Noun: and\n",
      "Sentence: The core problems of artificial intelligence include programming computers \n",
      "for certain traits such as knowledge, reasoning, problem solving, perception, learning, planning, \n",
      "and the ability to manipulate and move objects.\n",
      "Sentence: \n"
     ]
    }
   ],
   "source": [
    "#PERFORMING DEPENDENCY PARSING\n",
    "\n",
    "import re\n",
    "\n",
    "# Function to perform simple dependency parsing\n",
    "def simple_dependency_parsing(paragraph):\n",
    "    # Split paragraph into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split()\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        \n",
    "        # Find simple dependencies\n",
    "        for i, word in enumerate(words):\n",
    "            # Find subject-verb-object structures\n",
    "            if word.lower() in ['is', 'are', 'was', 'were']:\n",
    "                subject = words[i-1] if i-1 >= 0 else None\n",
    "                verb = word\n",
    "                obj = words[i+1] if i+1 < len(words) else None\n",
    "                print(f\"Subject: {subject}, Verb: {verb}, Object: {obj}\")\n",
    "            # Handle basic adjective-noun structures\n",
    "            if word.lower() in ['intelligent', 'technical', 'specialized', 'essential']:\n",
    "                adj = word\n",
    "                noun = words[i+1] if i+1 < len(words) else None\n",
    "                print(f\"Adjective: {adj}, Noun: {noun}\")\n",
    "\n",
    "# Sample paragraph (corpus)\n",
    "paragraph = \"\"\"\n",
    "Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines. \n",
    "It has become an essential part of the technology industry. Research associated with artificial intelligence \n",
    "is highly technical and specialized. The core problems of artificial intelligence include programming computers \n",
    "for certain traits such as knowledge, reasoning, problem solving, perception, learning, planning, \n",
    "and the ability to manipulate and move objects.\n",
    "\"\"\"\n",
    "\n",
    "# Perform simple dependency parsing\n",
    "simple_dependency_parsing(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c1f3d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Artificial intelligence is transforming technology.' => Category: Technology\n",
      "Sentence: 'Python is a popular programming language for data science.' => Category: Programming\n",
      "Sentence: 'Cybersecurity threats are increasing with digital transformation.' => Category: Technology\n",
      "Sentence: 'Business strategies evolve with market trends.' => Category: Business\n"
     ]
    }
   ],
   "source": [
    "#CLASSIFICATION OF CORPUS\n",
    "\n",
    "# Define categories and corresponding keywords\n",
    "categories = {\n",
    "    \"Technology\": [\"artificial intelligence\", \"machine learning\", \"technology\", \"digital transformation\"],\n",
    "    \"Programming\": [\"python\", \"programming language\", \"code\", \"developer\"],\n",
    "    \"Cybersecurity\": [\"cybersecurity\", \"security\", \"hacker\", \"data breach\"],\n",
    "    \"Business\": [\"business\", \"economics\", \"market\", \"finance\"]\n",
    "}\n",
    "\n",
    "# Function to classify a sentence into a category\n",
    "def classify_sentence(sentence):\n",
    "    for category, keywords in categories.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword in sentence.lower():\n",
    "                return category\n",
    "    return \"Unclassified\"  # Default category if no match found\n",
    "\n",
    "# Sample sentences to classify\n",
    "sentences = [\n",
    "    \"Artificial intelligence is transforming technology.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Cybersecurity threats are increasing with digital transformation.\",\n",
    "    \"Business strategies evolve with market trends.\"\n",
    "]\n",
    "\n",
    "# Classify each sentence\n",
    "for sentence in sentences:\n",
    "    category = classify_sentence(sentence)\n",
    "    print(f\"Sentence: '{sentence}' => Category: {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f230d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
